legend("topleft", legend = c("left:fore", "left:hind", "none", "right:fore", "right:hind"),fill=c(1,2,3,4,5))
#Correlation of PC3-PC4
plot(data$pc3, data$pc4, col = data$lameLeg, main = "Correlation of PC3 and PC4", xlab = "PC3", ylab = "PC4")
legend("topleft", legend = c("left:fore", "left:hind", "none", "right:fore", "right:hind"),fill=c(1,2,3,4,5))
# Correlation of right:fore and left:hind
plot(subset(data, lameLeg == "right:fore")$A, subset(data, lameLeg == "right:fore")$W)
par(mfrow=c(1,2))
#Correlation of AW
library("ggplot2")
require(gridExtra)
p1 <- ggplot(dDia,aes(x=A, y=W, group=lameLeg)) + geom_point() + geom_point(aes(color=lameLeg)) + scale_color_manual(values=c(6,3,7) )
#Correlation of PC
p2 <- ggplot(dDia,aes(x=pc3, y=pc4, group=lameLeg)) + geom_point() + geom_point(aes(color=lameLeg)) + scale_color_manual(values=c(6,3,7) )
grid.arrange(p1, p2, ncol=2)
p1 <- ggplot(dDia,aes(x=A, y=W, group=lameLeg)) + geom_point() + geom_point(aes(color=lameLeg)) + scale_color_manual(values=c(6,3,7) ) + ggtitle("helo")
#Correlation of PC
p2 <- ggplot(dDia,aes(x=pc3, y=pc4, group=lameLeg)) + geom_point() + geom_point(aes(color=lameLeg)) + scale_color_manual(values=c(6,3,7) )
grid.arrange(p1, p2, ncol=2)
p1 <- ggplot(dDia,aes(x=A, y=W, group=lameLeg)) + geom_point() + geom_point(aes(color=lameLeg)) + scale_color_manual(values=c(6,3,7) ) + ggtitle("Correlation between A and W")
#Correlation of PC
p2 <- ggplot(dDia,aes(x=pc3, y=pc4, group=lameLeg)) + geom_point() + geom_point(aes(color=lameLeg)) + scale_color_manual(values=c(6,3,7) ) + ggtitle("Correlation between PC3 and PC4")
grid.arrange(p1, p2, ncol=2)
p1 <- ggplot(dDia,aes(x=A, y=W, group=lameLeg)) + geom_point() + geom_point(aes(color=lameLeg)) + scale_color_manual(values=c(6,3,7) ) + ggtitle("Correlation of A and W")
#Correlation of PC
p2 <- ggplot(dDia,aes(x=pc3, y=pc4, group=lameLeg)) + geom_point() + geom_point(aes(color=lameLeg)) + scale_color_manual(values=c(6,3,7) ) + ggtitle("Correlation of PC3 and PC4")
grid.arrange(p1, p2, ncol=2)
View(p1)
guess
guess
(cm <- confusionMatrix(data=guess, reference = d$lameLeg))
(cm <- confusionMatrix(data=guess, reference = dDia$lameLeg))
(cm[1,1] + cm[2,2] + cm[3,3]) / sum(cm)
cm[1]
cm$table
(cm <- confusionMatrix(data=guess, reference = dDia$lameLeg)$table)
(cm[1,1] + cm[2,2] + cm[3,3]) / sum(cm)
setwd("/Users/frederikravnborg/Library/CloudStorage/OneDrive-DanmarksTekniskeUniversitet/DTU-Frederikâ€™s MacBook Pro/ProjStat/Project, Group/ProjStat")
#### Preparing data ####
# Import packages
library(e1071)
library(caTools)
library(class)
library(caret)
# Define data
# Collapse diagonals in data
dDia <- read.table("horse_data23.txt", header=TRUE, as.is=TRUE)
for (i in 1:85){
if(dDia$lameLeg[i] == "right:fore" || dDia$lameLeg[i] == "left:hind"){
dDia$lameLeg[i] <- "rflh"}
else if (dDia$lameLeg[i] == "left:fore" || dDia$lameLeg[i] == "right:hind"){
dDia$lameLeg[i] <- "lfrh"}
}
dDia$lameLeg <- as.factor(dDia$lameLeg)
d <- read.table("horse_data23.txt", header=TRUE, as.is=TRUE)
d$lameLeg <- as.factor(d$lameLeg)
horses <- list("B1","B2","B3","B4","B5","B6","B7","B9")
shapiro.test(d$A) # pval = 0.005048
shapiro.test(d$W) # pval = 0.1345
shapiro.test(d$S) # pval = 0.379
#### KNN AW ####
# KNN with optimal K
accAWm <- matrix(ncol=85, nrow=10)
guessm <- matrix(ncol=85, nrow=10)
for (K in 1:10){
accAW <- c()
guess <- c()
for (i in 1:8){
traincl <- subset(d, d$horse != horses[i])$lameLeg # train classes
trainD <- cbind(subset(d, d$horse != horses[i])$A, subset(d, d$horse != horses[i])$W)
testD  <- cbind(subset(d, d$horse == horses[i])$A, subset(d, d$horse == horses[i])$W)
classifier <- knn(train=trainD, test=testD, cl = traincl, k = K)
guess <- append(guess, classifier)
accAW <- c(accAW, as.integer(classifier == subset(d, d$horse == horses[i])$lameLeg))
}
accAWm[K,] <- accAW
}
accs <- c()
for (i in 1:10) { accs <- c(accs, mean(accAWm[i,])) }
(bestK <- which.max(accs))
accAW <- accAWm[bestK,]
(scoreAW <- mean(accAW))
AW_K <- bestK
#### KNN PC ####
# KNN with optimal K
accPCm <- matrix(ncol=85, nrow=10)
guessm <- matrix(ncol=85, nrow=10)
for (K in 1:10){
accPC <- c()
guess <- c()
for (i in 1:8){
traincl <- subset(d, d$horse != horses[i])$lameLeg # train classes
trainD <- cbind(subset(d, d$horse != horses[i])$pc3, subset(d, d$horse != horses[i])$pc4)
testD  <- cbind(subset(d, d$horse == horses[i])$pc3, subset(d, d$horse == horses[i])$pc4)
classifier <- knn(train=trainD, test=testD, cl = traincl, k = K)
guess <- append(guess, classifier)
accPC <- c(accPC, as.integer(classifier == subset(d, d$horse == horses[i])$lameLeg))
}
accPCm[K,] <- accPC
}
accs <- c()
for (i in 1:10) { accs <- c(accs, mean(accPCm[i,])) }
(bestK <- which.max(accs))
accPC <- accPCm[bestK,]
(scorePC <- mean(accPC))
PC_K <- bestK
#### KNN PC ####
# KNN with optimal K
accPCm <- matrix(ncol=85, nrow=10)
guessm <- matrix(ncol=85, nrow=10)
for (K in 1:10){
accPC <- c()
guess <- c()
for (i in 1:8){
traincl <- subset(d, d$horse != horses[i])$lameLeg # train classes
trainD <- cbind(subset(d, d$horse != horses[i])$pc3)
testD  <- cbind(subset(d, d$horse == horses[i])$pc3)
classifier <- knn(train=trainD, test=testD, cl = traincl, k = K)
guess <- append(guess, classifier)
accPC <- c(accPC, as.integer(classifier == subset(d, d$horse == horses[i])$lameLeg))
}
accPCm[K,] <- accPC
}
accs <- c()
for (i in 1:10) { accs <- c(accs, mean(accPCm[i,])) }
(bestK <- which.max(accs))
accPC <- accPCm[bestK,]
(scorePC <- mean(accPC))
PC_K <- bestK
#### KNN AWPC ####
# KNN with optimal K
accAWPCm <- matrix(ncol=85, nrow=10)
guessm <- matrix(ncol=85, nrow=10)
for (K in 1:10){
accAWPC <- c()
guess <- c()
for (i in 1:8){
traincl <- subset(d, d$horse != horses[i])$lameLeg # train classes
trainD <- cbind(subset(d, d$horse != horses[i])$A, subset(d, d$horse != horses[i])$W, subset(d, d$horse != horses[i])$pc3, subset(d, d$horse != horses[i])$pc4)
testD  <- cbind(subset(d, d$horse == horses[i])$A, subset(d, d$horse == horses[i])$W, subset(d, d$horse == horses[i])$pc3, subset(d, d$horse == horses[i])$pc4)
classifier <- knn(train=trainD, test=testD, cl = traincl, k = K)
guess <- append(guess, classifier)
accAWPC <- c(accAWPC, as.integer(classifier == subset(d, d$horse == horses[i])$lameLeg))
}
accAWPCm[K,] <- accAWPC
}
accs <- c()
for (i in 1:10) { accs <- c(accs, mean(accAWPCm[i,])) }
(bestK <- which.max(accs))
accAWPC <- accAWPCm[bestK,]
(scoreAWPC <- mean(accAWPC))
AWPC_K <- bestK
#### KNN DAW ####
d <- dDia
# KNN with optimal K
accDAWm <- matrix(ncol=85, nrow=10)
guessm <- matrix(ncol=85, nrow=10)
for (K in 1:10){
accDAW <- c()
guess <- c()
for (i in 1:8){
traincl <- subset(d, d$horse != horses[i])$lameLeg # train classes
trainD <- cbind(subset(d, d$horse != horses[i])$A, subset(d, d$horse != horses[i])$W)
testD  <- cbind(subset(d, d$horse == horses[i])$A, subset(d, d$horse == horses[i])$W)
classifier <- knn(train=trainD, test=testD, cl = traincl, k = K)
guess <- append(guess, classifier)
accDAW <- c(accDAW, as.integer(classifier == subset(d, d$horse == horses[i])$lameLeg))
}
accDAWm[K,] <- accDAW
}
accs <- c()
for (i in 1:10) { accs <- c(accs, mean(accDAWm[i,])) }
(bestK <- which.max(accs))
accDAW <- accDAWm[bestK,]
(scoreDAW <- mean(accDAW))
DAW_K <- bestK
#### KNN DPC ####
d <- dDia
# KNN with optimal K
accDPCm <- matrix(ncol=85, nrow=10)
guessm <- matrix(ncol=85, nrow=10)
for (K in 1:10){
accDPC <- c()
guess <- c()
for (i in 1:8){
traincl <- subset(d, d$horse != horses[i])$lameLeg # train classes
trainD <- cbind(subset(d, d$horse != horses[i])$pc3, subset(d, d$horse != horses[i])$pc4)
testD  <- cbind(subset(d, d$horse == horses[i])$pc3, subset(d, d$horse == horses[i])$pc4)
classifier <- knn(train=trainD, test=testD, cl = traincl, k = K)
guess <- append(guess, classifier)
accDPC <- c(accDPC, as.integer(classifier == subset(d, d$horse == horses[i])$lameLeg))
}
accDPCm[K,] <- accDPC
}
accs <- c()
for (i in 1:10) { accs <- c(accs, mean(accDPCm[i,])) }
(bestK <- which.max(accs))
accDPC <- accDPCm[bestK,]
(scoreDPC <- mean(accDPC))
DPC_K <- bestK
#### KNN DAWPC ####
d <- dDia
# KNN with optimal K
accDAWPCm <- matrix(ncol=85, nrow=10)
guessm <- matrix(ncol=85, nrow=10)
for (K in 1:10){
accDAWPC <- c()
guess <- c()
for (i in 1:8){
traincl <- subset(d, d$horse != horses[i])$lameLeg # train classes
trainD <- cbind(subset(d, d$horse != horses[i])$A, subset(d, d$horse != horses[i])$W, subset(d, d$horse != horses[i])$pc3, subset(d, d$horse != horses[i])$pc4)
testD  <- cbind(subset(d, d$horse == horses[i])$A, subset(d, d$horse == horses[i])$W, subset(d, d$horse == horses[i])$pc3, subset(d, d$horse == horses[i])$pc4)
classifier <- knn(train=trainD, test=testD, cl = traincl, k = K)
guess <- append(guess, classifier)
accDAWPC <- c(accDAWPC, as.integer(classifier == subset(d, d$horse == horses[i])$lameLeg))
}
accDAWPCm[K,] <- accDAWPC
}
accs <- c()
for (i in 1:10) { accs <- c(accs, mean(accDAWPCm[i,])) }
(bestK <- which.max(accs))
accDAWPC <- accDAWPCm[bestK,]
(scoreDAWPC <- mean(accDAWPC))
DAWPC_K <- bestK
#### KNN DAWPC3 ####
# Model, hvor PC4 er fjernet
d <- dDia
# KNN with optimal K
accDAWPC3m <- matrix(ncol=85, nrow=10)
guessm <- matrix(ncol=85, nrow=10)
for (K in 1:10){
accDAWPC3 <- c()
guess <- c()
for (i in 1:8){
traincl <- subset(d, d$horse != horses[i])$lameLeg # train classes
trainD <- cbind(subset(d, d$horse != horses[i])$A, subset(d, d$horse != horses[i])$W, subset(d, d$horse != horses[i])$pc3)
testD  <- cbind(subset(d, d$horse == horses[i])$A, subset(d, d$horse == horses[i])$W, subset(d, d$horse == horses[i])$pc3)
classifier <- knn(train=trainD, test=testD, cl = traincl, k = K)
guess <- append(guess, classifier)
accDAWPC3 <- c(accDAWPC3, as.integer(classifier == subset(d, d$horse == horses[i])$lameLeg))
}
accDAWPC3m[K,] <- accDAWPC3
}
accs <- c()
for (i in 1:10) { accs <- c(accs, mean(accDAWPC3m[i,])) }
(bestK <- which.max(accs))
accDAWPC3 <- accDAWPC3m[bestK,]
(scoreDAWPC3 <- mean(accDAWPC3))
DAWPC3_K <- bestK
#### Summary ####
# K-values
AW_K
PC_K
AWPC_K
DAW_K
DPC_K
DAWPC_K
# Accuracies
round(scoreAW,3)
round(scorePC,3)
round(scoreAWPC,3)
round(scoreDAW,3)
round(scoreDPC,3)
round(scoreDAWPC,3)
#### McNemar ####
# function to create f11, f01, f10, f00
f <- function(x,y){
f11 <- 0; f01 <- 0; f10 <- 0; f00 <- 0;
for (i in 1:length(x)){
if      (x[i] == 1 & y[i] == 1){f11 <- f11+1}
else if (x[i] == 0 & y[i] == 1){f01 <- f01+1}
else if (x[i] == 1 & y[i] == 0){f10 <- f10+1}
else if (x[i] == 0 & y[i] == 0){f00 <- f00+1}}
matrix(c(f11, f01, f10, f00),
nrow = 2,
dimnames = list("1st Model" = c("Correct", "Wrong"),
"2nd Model" = c("Correct", "Wrong")))}
#### McNemar KNN ####
A <- cbind(accAW,accPC,accAWPC,accDAW,accDPC,accDAWPC)
(mcMat <- data.frame(matrix(ncol = 6, nrow = 6)))
colnames(mcMat) <- c("AW","PC","AWPC","DAW","DPC","DAWPC")
rownames(mcMat) <- c("AW","PC","AWPC","DAW","DPC","DAWPC")
for (i in 1:6){
for (j in i:6){ if(i != j){
mcMat[i,j] <- p.adjust(mcnemar.test(f(A[,i], A[,j]))[3], method="BH", n=15)
}}
}
mcNemar_pvalues_KNN <- mcMat
signif(mcNemar_pvalues_KNN,2)
#### McNemar Decision Tree ####
CT <- t(read.table("CT_res.csv", sep=",", header = FALSE))
CTD <- t(read.table("CT_collapsed_res.csv", sep=",", header = FALSE))
A <- cbind(accAW,accPC,accAWPC,accDAW,accDPC,accDAWPC, CT[,1], CT[,2], CT[,3], CTD[,1], CTD[,2], CTD[,3])
(mcMat <- data.frame(matrix(ncol = 6, nrow = 6)))
colnames(mcMat) <- c("AW","PC","AWPC","DAW","DPC","DAWPC")
rownames(mcMat) <- c("AW","PC","AWPC","DAW","DPC","DAWPC")
for (i in 7:12){
for (j in i:12){ if(i != j){
mcMat[i-6,j-6] <- p.adjust(mcnemar.test(f(A[,i], A[,j]))[3], method="BH", n=15)
}}
}
mcNemar_pvalues_Tree <- mcMat
signif(mcNemar_pvalues_Tree,2)
#### McNemar both models ####
# With p-adjustment
(mcMat <- data.frame(matrix(ncol = 12, nrow = 12)))
colnames(mcMat) <- c("kAW","kPC","kAWPC","kDAW","kDPC","kDAWPC","tAW","tPC","tAWPC","tDAW","tDPC","tDAWPC")
rownames(mcMat) <- c("kAW","kPC","kAWPC","kDAW","kDPC","kDAWPC","tAW","tPC","tAWPC","tDAW","tDPC","tDAWPC")
for (i in 1:12){
for (j in i:12){ if(i != j){
mcMat[i,j] <- p.adjust(mcnemar.test(f(A[,i], A[,j]))[3],n=66)
}}
}
mcNemar_pvalues_Both <- mcMat
signif(mcNemar_pvalues_Both,2)
signif(mcNemar_pvalues_KNN,2)
signif(mcNemar_pvalues_Tree,2)
signif(mcNemar_pvalues_Both,3)
library(MASS)
write.matrix(signif(mcNemar_pvalues_Both,2),file="mcNemar_pvalues_Both.csv", sep=",")
write.matrix(signif(mcNemar_pvalues_Tree,2),file="mcNemar_pvalues_Tree.csv", sep=",")
write.matrix(mcNemar_pvalues_KNN,file="mcNemar_pvalues_KNN.csv", sep=",")
(accDT <- round(c(mean(CT[,1]), mean(CT[,2]), mean(CT[,3]), mean(CTD[,1]), mean(CTD[,2]), mean(CTD[,3])),3))
(cm <- confusionMatrix(data=guess, reference = dDia$lameLeg)$table)
(cm[1,1] + cm[2,2] + cm[3,3]) / sum(cm)
(cm <- confusionMatrix(data=guess, reference = dDia$lameLeg)$table)
(cm[1,1] + cm[2,2] + cm[3,3]) / sum(cm)
#### KNN DAWPC ####
d <- dDia
# KNN with optimal K
accDAWPCm <- matrix(ncol=85, nrow=10)
guessm <- matrix(ncol=85, nrow=10)
for (K in 1:10){
accDAWPC <- c()
guess <- c()
for (i in 1:8){
traincl <- subset(d, d$horse != horses[i])$lameLeg # train classes
trainD <- cbind(subset(d, d$horse != horses[i])$A, subset(d, d$horse != horses[i])$W, subset(d, d$horse != horses[i])$pc3, subset(d, d$horse != horses[i])$pc4)
testD  <- cbind(subset(d, d$horse == horses[i])$A, subset(d, d$horse == horses[i])$W, subset(d, d$horse == horses[i])$pc3, subset(d, d$horse == horses[i])$pc4)
classifier <- knn(train=trainD, test=testD, cl = traincl, k = K)
guess <- append(guess, classifier)
accDAWPC <- c(accDAWPC, as.integer(classifier == subset(d, d$horse == horses[i])$lameLeg))
}
accDAWPCm[K,] <- accDAWPC
}
accs <- c()
for (i in 1:10) { accs <- c(accs, mean(accDAWPCm[i,])) }
(bestK <- which.max(accs))
accDAWPC <- accDAWPCm[bestK,]
(scoreDAWPC <- mean(accDAWPC))
DAWPC_K <- bestK
(cm <- confusionMatrix(data=guess, reference = dDia$lameLeg)$table)
(cm[1,1] + cm[2,2] + cm[3,3]) / sum(cm)
guess
guess
guessm
classifier
#for (K in 1:10){
K <- 1
accDAWPC <- c()
guess <- c()
for (i in 1:8){
traincl <- subset(d, d$horse != horses[i])$lameLeg # train classes
trainD <- cbind(subset(d, d$horse != horses[i])$A, subset(d, d$horse != horses[i])$W, subset(d, d$horse != horses[i])$pc3, subset(d, d$horse != horses[i])$pc4)
testD  <- cbind(subset(d, d$horse == horses[i])$A, subset(d, d$horse == horses[i])$W, subset(d, d$horse == horses[i])$pc3, subset(d, d$horse == horses[i])$pc4)
classifier <- knn(train=trainD, test=testD, cl = traincl, k = K)
guess <- append(guess, classifier)
accDAWPC <- c(accDAWPC, as.integer(classifier == subset(d, d$horse == horses[i])$lameLeg))
}
accDAWPC
(cm <- confusionMatrix(data=guess, reference = dDia$lameLeg)$table)
(cm[1,1] + cm[2,2] + cm[3,3]) / sum(cm)
d <- dDia
# KNN with optimal K
accDAWPCm <- matrix(ncol=85, nrow=10)
guessm <- matrix(ncol=85, nrow=10)
#for (K in 1:10){
K <- 1
accDAWPC <- c()
guess <- c()
true <- c()
for (i in 1:8){
traincl <- subset(d, d$horse != horses[i])$lameLeg # train classes
trainD <- cbind(subset(d, d$horse != horses[i])$A, subset(d, d$horse != horses[i])$W, subset(d, d$horse != horses[i])$pc3, subset(d, d$horse != horses[i])$pc4)
testD  <- cbind(subset(d, d$horse == horses[i])$A, subset(d, d$horse == horses[i])$W, subset(d, d$horse == horses[i])$pc3, subset(d, d$horse == horses[i])$pc4)
classifier <- knn(train=trainD, test=testD, cl = traincl, k = K)
guess <- append(guess, classifier)
true <- append(true, subset(d, d$horse == horses[i])$lameLeg)
accDAWPC <- c(accDAWPC, as.integer(classifier == subset(d, d$horse == horses[i])$lameLeg))
}
accDAWPCm[K,] <- accDAWPC
#}
(cm <- confusionMatrix(data=guess, reference = true)$table)
(cm[1,1] + cm[2,2] + cm[3,3]) / sum(cm)
plot_confusion_matrix(
cm,
target_col = "Actual",
prediction_col = "Prediction",
counts_col = "N",
class_order = NULL,
add_sums = FALSE,
add_counts = TRUE,
add_normalized = TRUE,
add_row_percentages = TRUE,
add_col_percentages = TRUE,
diag_percentages_only = FALSE,
rm_zero_percentages = TRUE,
rm_zero_text = TRUE,
add_zero_shading = TRUE,
add_arrows = TRUE,
counts_on_top = FALSE,
palette = "Blues",
intensity_by = "counts",
theme_fn = ggplot2::theme_minimal,
place_x_axis_above = TRUE,
rotate_y_text = TRUE,
digits = 1,
font_counts = font(),
font_normalized = font(),
font_row_percentages = font(),
font_col_percentages = font(),
arrow_size = 0.048,
arrow_nudge_from_text = 0.065,
tile_border_color = NA,
tile_border_size = 0.1,
tile_border_linetype = "solid",
sums_settings = sum_tile_settings(),
darkness = 0.8
)
library(cvms)
plot_confusion_matrix(
cm,
target_col = "Actual",
prediction_col = "Prediction",
counts_col = "N",
class_order = NULL,
add_sums = FALSE,
add_counts = TRUE,
add_normalized = TRUE,
add_row_percentages = TRUE,
add_col_percentages = TRUE,
diag_percentages_only = FALSE,
rm_zero_percentages = TRUE,
rm_zero_text = TRUE,
add_zero_shading = TRUE,
add_arrows = TRUE,
counts_on_top = FALSE,
palette = "Blues",
intensity_by = "counts",
theme_fn = ggplot2::theme_minimal,
place_x_axis_above = TRUE,
rotate_y_text = TRUE,
digits = 1,
font_counts = font(),
font_normalized = font(),
font_row_percentages = font(),
font_col_percentages = font(),
arrow_size = 0.048,
arrow_nudge_from_text = 0.065,
tile_border_color = NA,
tile_border_size = 0.1,
tile_border_linetype = "solid",
sums_settings = sum_tile_settings(),
darkness = 0.8
)
plot_confusion_matrix(
cm,
target_col = "Actual",
prediction_col = "Prediction",
counts_col = "N")
plot_confusion_matrix(
cm,
target_col = "Actual",
prediction_col = "Prediction",
counts_col = "N")
library(ggplot2)
plot_confusion_matrix(
cm,
target_col = "Actual",
prediction_col = "Prediction",
counts_col = "N")
library(cvms)
library(ggplot2)
install.packages(cvms)
install.packages("cvms")
library(cvms)
library(ggplot2)
plot_confusion_matrix(
cm,
target_col = "Actual",
prediction_col = "Prediction",
counts_col = "N")
(cm <- data.frame(confusionMatrix(data=guess, reference = true)$table))
(cm <- data.frame(confusionMatrix(data=guess, reference = true)$table))
plot_confusion_matrix(
cm,
target_col = "Actual",
prediction_col = "Prediction",
counts_col = "N")
(cm <- confusionMatrix(data=guess, reference = true)$table)
(cm <- matrix(c(27,3,0,0,20,3,0,2,30), nrow=3))
(cm <- data.frame(matrix(c(27,3,0,0,20,3,0,2,30), nrow=3)))
colnames(cm) <- c("lfrh", "none", "rhlh")
rownames(cm) <- c("lfrh", "none", "rhlh")
cm
library(cvms)
library(ggplot2)
plot_confusion_matrix(
cm,
target_col = "Actual",
prediction_col = "Prediction",
counts_col = "N")
(cm <- confusionMatrix(data=guess, reference = true)$table)
